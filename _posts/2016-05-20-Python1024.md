---
layout: post
title: 使用Python爬取1024上的图片
comments: true
categories: 
- Python
- 爬虫
---  

&#160; &#160; &#160;&#160;初学Python，用几天时间看完了[Python简明教程](http://www.cnblogs.com/txw1958/archive/2012/12/10/A_Byte_of_Python3.html)和廖雪峰的[Python教程](http://www.liaoxuefeng.com/wiki/0014316089557264a6b348958f449949df42a6d3a2e542c000)，加上一些用Python3写的爬虫的例子，开始自己动手写了一个爬虫来爬取1024的图片，下面是具体分析：  
&#160; &#160; &#160;&#160;- 首先我们确定要爬取的网址：打开`1024bug.com`，进入子站点->图片区，我这里确定的网址为
`http://cl.llluuu.me/thread0806.php?fid=16`（需要代理访问） 当然可能当你看到这篇博客的时候这个网址已经打不开了，重新打开主站便是，我们的目的是爬取这个网站上所有标亮的标题下的图片。  
&#160; &#160; &#160;&#160;- 观察网站的Html结构，我们发现每条目录都是这样的结构:  

```html
<tr align="center" class="tr3 t_one"></tr>
<tr align="center" class="tr3 t_one"></tr>
<tr align="center" class="tr3 t_one"></tr>
```

&#160; &#160; &#160;&#160;- 具体到每个标题的链接：

```html
<a href="htm_data/16/1402/1430628.html" target="_blank" id=""><b><font color="blue">图区QQ视频广告及非法图片举报贴</font></b></a>
<a href="htm_data/16/1605/1931337.html" target="_blank" id=""><font color="green">和|谐[29P]</font></a>
<a href="htm_data/16/1605/1939712.html" target="_blank" id="">和|谐[10P]</a>
```
&#160; &#160; &#160;&#160;可以看出，我们要筛选保存的链接前面有一个与众不同的地方`<font  color="green">`，至此我们便可以开始编写代码了。  

- 第一步，我们获取首页中所有需要的标题的链接  

```python
def getContant(Weburl):
#保存网站的html内容
    Webheader= {'Upgrade-Insecure-Requests':'1',
                'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.103 Safari/537.36',}
    req = urllib.request.Request(url = Weburl,headers=Webheader)
    respose = urllib.request.urlopen(req)
    _contant = respose.read()
    respose.close()
    return str(_contant)
    

def getUrl(URL):
#URL = 'http://cl.llluuu.me/thread0806.php?fid=16&search=&page='
    #pageIndex = input("要下载多少页呢？\n")
    pageIndex = 1
    for i in range(1,int(pageIndex)+1):
        Weburl = URL + str(i)
        #Weburl为首页URL，由于之前我想下载多页内容，所以用了这种写法
        contant = getContant(Weburl)
        comp = re.compile(r'<a href="htm_data.{0,30}html" target="_blank" id=""><font color=g')
        #匹配所有含有<font color="green">的链接
        #这里筛选了两次
        urlList1 = comp.findall(contant)
        comp = re.compile(r'a href="(.*?)"')
        urlList2 = comp.findall(str(urlList1))
        urlList = []
        #因为获得的Url为htm/231202/....html的形式，要给它补全
        for url1 in urlList2:
            url2 = 'http://cl.llluuu.me/'+url1
            urlList.append(url2)
        return urlList
```
&#160; &#160; &#160;&#160;测试一下: `UrlList = getUrl(URL)`  `print(UrlList)`  
&#160; &#160; &#160;&#160;输出为 :
`['http://cl.llluuu.me/htm_data/16/1605/1926705.html',`
`'http://cl.llluuu.me/htm_data/16/1605/1934034.html', ...]`  
至此，我们成功取得的每个目录的链接的集合：`UrlList`
我们还需要一个方法，来命名已经下载的图片，同时需要一个文件来保持所有已经下载的图片名字，这样在下一次运行程序时，就不会重复下载。  

下面是一些方法：  

1.获取图片下载地址（需要再次使用正则匹配）：  

```Python
for url1 in UrlList:
        contant = getContant(url1)
        comp = re.compile(r'<input src=\\\'https(.*?)\\\' type=\\\'i')
        url2 = comp.findall(contant)
        for url3 in url2:
            url3 = 'https'+url3
            print(url3)
```

2.命名要下载的图片：  

```python
def finalName(path):
    if not os.path.isdir(targetDir):
        os.mkdir(targetDir)
    pos = path.rindex('/')
    t = os.path.join(targetDir, path[pos+1:])
    return t
```

3.CallBack函数，显示下载进度:  

```python
def callback(a,b,c):
    per = 100.0*a*b/c
    if(per>100):
        per = 100
    print("%.2f%%"%per,end='\r')
    sys.stdout.flush()
```

4.打开一个DownloadList文件来保存已经下载的图片链接：  

```python
filename = 'downloadList.txt'
with open(filename, 'ab') as f:
    f.close()
if os.path.getsize(filename) > 0:
    print('载入目录中........')
    with open(filename, 'rb') as files:
        downloadList = pickle.load(files)
else:
    print('新建了目录')
    downloadList = []
    with open(filename, 'wb') as files:
        pickle.dump(downloadList, files)
    with open(filename, 'rb') as files:
        downloadList = pickle.load(files)
```

嗯...说的不是很清楚，下面是完整的代码：  

```python
import urllib.request
import re
import sys
import os
import pickle
targetDir = r"F:\Python\羞羞的图片"  #文件保存路径
def finalName(path):
    if not os.path.isdir(targetDir):
        os.mkdir(targetDir)
    pos = path.rindex('/')
    t = os.path.join(targetDir, path[pos+1:])
    return t
def callback(a,b,c):
    per = 100.0*a*b/c
    if(per>100):
        per = 100
    print("%.2f%%"%per,end='\r')
    sys.stdout.flush()
def getContant(Weburl):
    Webheader= {'Upgrade-Insecure-Requests':'1',
                'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.103 Safari/537.36',}
    req = urllib.request.Request(url = Weburl,headers=Webheader)
    respose = urllib.request.urlopen(req)
    _contant = respose.read()
    respose.close()
    return str(_contant)
def getUrl(URL):
    #pageIndex = input("要下载多少页呢？\n")
    pageIndex = 1
    for i in range(1,int(pageIndex)+1):
        Weburl = URL + str(i)#获取每大页的URL
        contant = getContant(Weburl)
        comp = re.compile(r'<a href="htm_data.{0,30}html" target="_blank" id=""><font color=g')
        urlList1 = comp.findall(contant)
        comp = re.compile(r'a href="(.*?)"')
        urlList2 = comp.findall(str(urlList1))
        urlList = []
        for url1 in urlList2:
            url2 = 'http://cl.llluuu.me/'+url1
            urlList.append(url2)
        return urlList
def download(UrlList):
    cal= 1
    pas = 1
    fail = 1
    filename = 'downloadList.txt'
    with open(filename, 'ab') as f:
        f.close()
    if os.path.getsize(filename) > 0:
        print('载入目录中........')
        with open(filename, 'rb') as files:
            downloadList = pickle.load(files)
    else:
        print('新建了目录')
        downloadList = []
        with open(filename, 'wb') as files:
            pickle.dump(downloadList, files)
        with open(filename, 'rb') as files:
            downloadList = pickle.load(files)
    for url1 in UrlList:
        contant = getContant(url1)
        comp = re.compile(r'<input src=\\\'https(.*?)\\\' type=\\\'i')
        url2 = comp.findall(contant)
        for url3 in url2:
            url3 = 'https'+url3
            print(url3)
            global cal
            global pas
            global downloadList
            global fail
            pos = url3.rindex('/')
            name = url3[pos + 1:]
            try:
                if name not in downloadList:
                    print('正在下载第' + str(cal) + '张图片')
                    urllib.request.urlretrieve(url3, finalName(url3), callback)
                    cal+=1
                    downloadList.append(name)
                    with open('F:/python/downloadList.txt', 'wb') as f:
                        pickle.dump(downloadList,f)
                else:
                    print('图片已经存在，已跳过'+str(pas)+'张')
                    pas += 1
            except urllib.error.URLError:
                print(str(fail)+'张下载失败')
URL = 'http://cl.llluuu.me/thread0806.php?fid=16&search=&page='
os.system('cls')
print('爬虫启动中，爬取网址为：'+URL)
UrlList = getUrl(URL)
print(UrlList)
download(UrlList)
print("所有标亮内容已经全部下载")
```
