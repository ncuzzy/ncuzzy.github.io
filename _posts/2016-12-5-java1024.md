---
layout: post
title: 用Java再次写个1024的爬虫
comments: true
categories: 
- Java
- 爬虫
---  
还有什么爬虫能比爬1024更刺激呢~爬取图片的用Python写过了，这次用Java写一个爬小说的。原本想写一个功能全面的能爬整站的爬虫，但都只是一些正则表达式的区别，写到后面慢慢失去了写下去的兴趣，毕竟这东西只能自己一个人写着玩。不过整体的框架写完了，用Java写比起用Python还是舒服多了的。  

### 1. 结构设计  
首先我们先把爬虫大体的框架想好，软件用的PowerDesigner，不要吐槽各种地方乱用，我只是把它当成一个画板qwq~  
User Case图  

![](http://o9py5j033.bkt.clouddn.com/usercase.png)  
类图  

![](http://o9py5j033.bkt.clouddn.com/class.png)

### 2. 基类  
先把基类撸出来  

    public class urlHandle {
        private String url;
        private String normalRegex;
        private String urlRegex;
        private String titleRegex;
        private String context;
        private String domain;
        
        urlHandle(){
            url = "";
            normalRegex = "";
            urlRegex = "";
            titleRegex = "";
            context = "";
            domain = "";
        }
        
        protected void setUrl(String s){
            url = s;
        }
        protected void setNormalRegex(String s){
            normalRegex = s;
        }
        protected void setUrlRegex(String s){
            urlRegex = s;
        }
        protected void setTitleRegex(String s){
            titleRegex = s;
        }
        protected void showContext(){
            System.out.println(context);
        }
        protected void setContent() throws Exception{
            assert(url!="");
            this.context = this.getContent(url); 
        }
        protected String showDomainName(){
            return domain;
        }
        //添加几个键值，让爬虫能顺利打开目标网站
        protected String getContent(String s) throws Exception{
            URL url = new URL(s);
            String Context = "";
            URLConnection conn = url.openConnection();
            conn.setRequestProperty("Connection", "Keep-Alive"); 
            conn.setRequestProperty("User-Agent", "Mozilla/5.0 (Windows NT 10.0; Win64; x64) Appl"
                    * "eWebKit/537.36 (KHTML, like Gecko) Chrome/54.0.2840.99 Safari/537.36");
            conn.setRequestProperty("Accept", "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8"); 
            conn.connect();
            InputStream is = conn.getInputStream();
            Context = IOUtils.toString(is, "gb2312");
            is.close();
            return Context;
        };
        
        /*图片，小说，电影，页面的结构都差不多，三个正则足够提取出需要的内容了
        **为了省去每个类重写这个方法的代码，直接写在基类里
        */regex1 提取整个<a></a>  regex2提取url regex3提取标题
        protected Map<String,String> getMap(){
            assert(domain!="" && context!="");
            String str = context;
            StringBuffer strTmp = new StringBuffer();
            Map<String, String> map = new HashMap<String, String>();
            Pattern regex1Hander = Pattern.compile(normalRegex);
            Pattern urlHander = Pattern.compile(urlRegex);
            Pattern titleHander = Pattern.compile(titleRegex);
            Matcher m = regex1Hander.matcher(str);
            while(m.find()){
                strTmp.append(m.group());
            };
            Matcher url = urlHander.matcher(strTmp);
            Matcher title = titleHander.matcher(strTmp);
            while(url.find()&&title.find()){
                map.put(domain+url.group(),title.group());
            }
            return map;
        };
        //原本想将访问过的URL存到数据库里面，所以多了这几个函数。
        protected boolean isExist(){
            
            return true;
        };
        
        protected boolean Download(){
        
            return true;
        };
        
        protected boolean Save(){
        
            return true;
        };
        //获取一级域名
        protected boolean getDomainName(){
            assert(url!=null&&url!="");
            //String s = "http://xiaohai.ga/thread0806.php?fid=20";
            Pattern p = Pattern.compile("\\w+?://\\w+?\\.\\w{1,5}/");
            Matcher m = p.matcher(url);
            if(m.find()){
                domain = m.group();
                return true;
            }   
            else 
                return false;
        }
        //测试方法，读取手动保存的网页测试正则，每次都连接太慢了
        public static StringBuffer readFile(String fileName) throws FileNotFoundException {
            File file = new File(fileName);
            StringBuffer str = new StringBuffer();
            String tmp = "";
            Scanner input = new Scanner(file);
            while (input.hasNext()) {
                tmp = input.nextLine();
                str.append(tmp);
            }
            input.close();
            return str;
        }
    }

### 3. FictionSearch  

    public class FictionSearch extends urlHandle{
        FictionSearch(){
            super();
            //定义了小说页面需要的正则
            super.setNormalRegex("<a href=\"htm.{0,50}id=\"\">[\u4e00-\u9fa5]+?<");
            super.setUrlRegex("htm_data/[0-9]+?/[0-9]+?/[0-9]+?\\.html");
            super.setTitleRegex("[\u4e00-\u9fa5]{1,50}");
        }
    }  

### 4. FictionDownload

    public class FictionDownload extends FictionSearch{
        private Map<String,String> map = new HashMap<>();
        
        FictionDownload() throws Exception{
            //没有指定页面URL
            System.out.println("No url intended");
        }
        
        FictionDownload(String s) throws Exception{
            super();
            super.setUrl(s);
            super.setContent();
            super.getDomainName();
            map = super.getMap();
        }
        
        public void download() throws Exception{
            for(String url:map.keySet()){
                if(!isExist(url)){
                    String title = map.get(url);
                    String path = "Fictions/"+title+".txt";
                    String fictionText = getTotalFiction(url);
                    File file = new File(path);
                    //测试用，因为没有写isExist...
                    if(file.exists())
                        System.out.println("Something Wrong,Fiction rewrited");
                    file.createNewFile();
                    BufferedWriter bw = new BufferedWriter(new FileWriter(file.getAbsoluteFile()));
                    bw.write(fictionText);
                    //关闭时强制将缓冲区数据写入内存，否则需要加上bw.flush();
                    bw.close();
                    System.out.println("文件保存在: "+file.getAbsolutePath());
                }
            }
        }
        
        protected boolean isExist(String url){
            return false;
        }
        
        private String getTid(String url){
            String[] t = url.replace(".html", "").replaceAll(".htm", "").split("/");
            String tid = t[t.length-1];
            return tid;
        }
        
        private String getTotalFiction(String url) throws Exception{
            String tid = this.getTid(url);
            String FirstPageText = getFictionText(url);
            String PageUrl = "";
            StringBuffer FictionText = new StringBuffer(FirstPageText);
            String domain = super.showDomainName();
            int i = 2;
            while(true){
                /*当时直接认为小说内容的第二页会和第一页是一样的结构，结果表面我还是太天真了,所以我这样是获取不到第二页的小说内容的，以后有心情再完善吧*/
                PageUrl = domain+"read.php?tid="+tid+"&page="+i;
                System.out.println(PageUrl);
                String pageText = this.getFictionText(PageUrl);
                if(pageText.length()<10)
                    break;
                else{
                    FictionText.append(pageText);
                    i++;
                }
            }
            return FictionText.toString();
        }
        
        private String getFictionText(String url) throws Exception{
            String Page = super.getContent(url);
            String ParaText = "";
            String PageText = "";
            Page = Page.replaceAll("<br>", "br");
            Page = Page.replaceAll("&nbsp;", "");
            Pattern p = Pattern.compile("tpc_content do_not_catch\">(.*?)<");
            Matcher m = p.matcher(Page);
            while(m.find()){
                ParaText = m.group(1);
                if(ParaText.length()<20)
                    continue;
                else
                    PageText += ParaText;
            }
            PageText = PageText.replaceAll("br", System.getProperty("line.separator"));
            return PageText;
        }
    }
### Main  

    public class Main {
        public static void main(String[] args) throws Exception {
            String ss = "http://xiaohai.ga/thread0806.php?fid=20";
            FictionDownload fd = new FictionDownload(ss);
            try {
                fd.download();
            } catch (Exception e) {
                e.printStackTrace();
            }
        }
    }

### 结果  
好吧这是唯一有看点的东西了...没有开线程，跟之前Python爬图片的比起来真的好慢，因为我在图书馆借了`Java并发编程实战`，然而还没翻开...
![](http://o9py5j033.bkt.clouddn.com/fictuon.png)

### 一点点总结  
学编程，最好是不要Copy and Paste from Stackoverflow...  
对语言有个大体的了解，遇到要使用的方法，类，库，先去看看官方文档，由大到小，由浅到深，很多博客其实看起来没有官方的舒服，比如Java API Docs，Beautiful Soup Documentation ，JavaFX API Documentation，查起来方便，结构也有序。
没事多写点有意思的小程序，保持热情。  
期末了，希望有机会能把那个获取斗鱼弹幕获取的程序写了，曾经用C++写过但连接成功登入成功获取数据失败，很无奈啊，上了一个学期计网，争取用Java把它搞定。