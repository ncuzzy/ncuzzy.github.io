---
layout: post
title: 1024bug爬虫多进程优化版
comments: true
categories: 
- 爬虫
- Python
---
一直纠结与怎么处理多进程同时写入文件的问题，用`while(i>0);`的方式觉得太不科学了，有没有找到具体的`muiltprocess.Lock`的说明，最终采取了每个进程返回多个`list`，最后一次性写入的解决方法，只是这样就不能让程序运行到中途退出了，否则下一次文件又会重新下载。  
下面是代码：  

```python
import urllib.request
import socket
import re
import sys
import os
import time
import json
from multiprocessing import Pool

def finalName(path):
    '''为下载的图片命名'''
    targetDir = r"F:\Python\1024BUG\羞羞的图片"  # 文件保存路径
    if not os.path.isdir(targetDir):
        os.mkdir(targetDir)
    pos = path.rindex('/')
    t = os.path.join(targetDir, path[pos+1:])
    return t

def callback(a,b,c):
    per = 100.0*a*b/c
    if(per>100):
        per = 100
    print("%.2f%%"%per,end='\r')
    sys.stdout.flush()

def getContant(Weburl):
    Webheader= {'Upgrade-Insecure-Requests':'1',
                'User-Agent':'Mozilla/5.0 (Windows NT 10.0; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/48.0.2564.103 Safari/537.36',
                'Accept': 'text / html, application / xhtml + xml, application / xml;q = 0.9, image / webp, * / *;q = 0.8',
                'Accept - Encoding':'gzip, deflate, sdch',
                'Accept - Language':'zh - CN, zh;q = 0.8',
                'Host': 'cl.llluuu.me',
                'Proxy - Connection': 'keep - alive'
    }
    req = urllib.request.Request(url = Weburl,headers=Webheader)
    respose = urllib.request.urlopen(req)
    _contant = respose.read()
    respose.close()
    return str(_contant)

def getPageUrl(URL):
    #pageIndex = input("要下载多少页呢？\n")
    pageIndex = 1
    for i in range(1,int(pageIndex)+1):
        Weburl = URL + str(i)#获取每大页的URL
        contant = getContant(Weburl)
        comp = re.compile(r'<a href="htm_data.{0,30}html" target="_blank" id=""><font color=g')
        urlList1 = comp.findall(contant)
        comp = re.compile(r'a href="(.*?)"')
        urlList2 = comp.findall(str(urlList1))
        urlList = []
        for url1 in urlList2:
            url2 = sURL+url1
            urlList.append(url2)
        return urlList

def initFile():
    with open(errorFilename, 'a') as f:
        if os.path.getsize(errorFilename) > 0:
            with open(errorFilename, 'r') as files:
                errorListJson = json.load(files)
                errorList = json.loads(errorListJson)
        else:
            errorList = []
    with open(filename, 'a') as f:
        if os.path.getsize(filename) > 0:
            print('载入目录中........')
            with open(filename, 'r') as files:
                downloadList_json = json.load(files)
                downloadList = json.loads(downloadList_json)
        else:
            print('新建了目录')
            downloadList = []
    return downloadList,errorList

def downImgList(UrlList,i,downloadList,errorList):
    print('第%d个进程，PID=%s ················已经创建'%(i,os.getpid()))
    succ, pas, fail = 1, 1, 1
    muiltDownList,muiltErrList = [],[]
    comp = re.compile(r'<input src=\\\'https(.*?)\\\' type=\\\'i')
    try:
        contant = getContant(UrlList[i])
    except socket.timeout:
        print("第%d个进程，PID=%s ················目录打开失败，跳过\n"%(i,os.getpid()))
        return
    imgList = comp.findall(contant)
    print('第%d个进程，PID=%s ················已经成功取得图片目录'%(i,os.getpid()))
    for temp in imgList:
        url = 'https' + temp
        pos = url.rindex('/')
        name = url[pos + 1:]
        try:
            if name in errorList:
                print('第%d个进程，PID=%s ················图片存在于错误列表中，已跳过'%(i,os.getpid()) + str(pas) + '张')
                pas += 1
            elif name not in downloadList:
                print(url)
                print('第%d个进程，PID=%s ················正在下载第'%(i,os.getpid()) + str(succ) + '张图片')
                urllib.request.urlretrieve(url,finalName(url), callback)
                succ += 1
                print('第%d个进程，PID=%s ················下载完成'%(i,os.getpid()))
                muiltDownList.append(name)

            else:
                print('第%d个进程，PID=%s ················图片已经存在，已跳过'%(i,os.getpid()) + str(pas))
                pas += 1
        except (urllib.request.URLError, socket.timeout, urllib.request.HTTPError) as e:
            print(e)
            print('第%d个进程，PID=%s ················'%(i,os.getpid()) + str(fail) + '张下载失败')
            fail += 1
            muiltErrList.append(name)
    print('第%d个进程，PID=%s ················'%(i,os.getpid())+'成功退出\n')
    return muiltDownList,muiltErrList,succ,pas,fail

def Download(UrlList):
    downloadList, errorList = initFile()
    finalList,finalDownloadList,finalErrorList = [],[],[]
    succ, pas, fail = 1, 1, 1
    P = Pool(4)
    for i in range(len(UrlList)):
        finalList.append(P.apply_async(downImgList,(UrlList,i,downloadList,errorList)))
    P.close()
    P.join()
    for list in finalList:
        finalDownloadList += list.get()[0]
        finalErrorList += list.get()[1]
        succ += list.get()[2]
        pas += list.get()[3]
        fail += list.get()[4]
    finalDownloadList += downloadList
    finalErrorList += errorList
    print('开始写入数据\n')
    with open(filename, 'w') as f:
        downloadList_json = json.dumps(finalDownloadList)
        json.dump(downloadList_json,f)
    with open(errorFilename, 'w') as f:
        errorJson = json.dumps(finalErrorList)
        json.dump(errorJson, f)
    print('写入成功')
    print('运行完毕----------------------')
    return succ,pas,fail

def logs(a,b,c):
    global times
    timeNow = time.strftime("%Y-%m-%d %H:%M:%S",time.localtime())
    log =r'''
-------------------------------------------------------------------------------------
     %s        本次更新用时 %ds 下载成功 %d 张，下载失败 %d 张，跳过 %d 张
-------------------------------------------------------------------------------------
'''%(timeNow,times,a,c,b)
    with open('logs.txt','a') as f:
        f.write(log)

if __name__ == "__main__":
    URL = 'http://cl.llluuu.me/thread0806.php?fid=16&search=&page='
    sURL = 'http://cl.llluuu.me/'
    filename = '1024DownloadList.txt'
    errorFilename = '1024ErrorLinks.txt'
    sTime = time.time()
    socket.setdefaulttimeout(10.0) #设置超时时间
    os.system('cls')
    print('爬虫启动中...............')
    a,b,c = Download(getPageUrl(URL))
    eTime = time.time()
    times = int(eTime-sTime)
    logs(a,b,c)
    print("所有内容已经全部下载")
```

图片不好放上来，直接复制了logs里的内容：  

     2016-05-26 18:33:15        本次更新用时 2352s 下载成功 126 张，下载失败 17 张，跳过 117 张

     2016-05-26 20:04:38        本次更新用时 325s 下载成功 2 张，下载失败 1 张，跳过 247 张

     2016-05-26 20:09:34        本次更新用时 295s 下载成功 1 张，下载失败 1 张，跳过 256 张

     2016-05-30 20:06:17        本次更新用时 91s 下载成功 33 张，下载失败 33 张，跳过 492 张

     2016-05-30 20:13:21        本次更新用时 92s 下载成功 33 张，下载失败 33 张，跳过 492 张
  
可以看到速度明显加快。
